{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOmE5ZMWgworYMEkVdsSRbT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hechubo/DDM-Coursework/blob/master/5055_1_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import subprocess\n",
        "import os\n",
        "import struct\n",
        "import json\n",
        "\n",
        "\n",
        "class Node:\n",
        "    '''\n",
        "    Node template for nodes in computational graph.\n",
        "\n",
        "    Attributes:\n",
        "        name (string): name for the node;\n",
        "        parameters (list of ndarray): list used to save node's parameters;\n",
        "        parameters_deltas (list of ndarray): list of gradients.\n",
        "    '''\n",
        "    def __init__(self, name, parameters=None):\n",
        "        self.name = name\n",
        "        self.parameters = parameters\n",
        "        self.parameters_deltas = [None for _ in range(len(self.parameters))]\n",
        "\n",
        "\n",
        "class Linear(Node):\n",
        "    '''\n",
        "    Linear layer\n",
        "    '''\n",
        "    def __init__(self, input_shape, output_shape, weight=None, bias=None):\n",
        "        '''\n",
        "        Args:\n",
        "            input_shape (int): input shape;\n",
        "            output_shape (int): output shape;\n",
        "            x (2darray): input array of shape (batch_size, num_pixels).\n",
        "        '''\n",
        "        if weight is None:\n",
        "            # Xavier initialization\n",
        "            weight = np.random.randn(input_shape, output_shape) * np.sqrt(2.0 / (input_shape + output_shape))\n",
        "        if bias is None:\n",
        "            bias = np.zeros(output_shape)\n",
        "        super(Linear, self).__init__('linear', [weight, bias])\n",
        "\n",
        "    def forward(self, x):\n",
        "        '''\n",
        "        Args:\n",
        "            x (2darray): input array of shape (batch_size, n_variables).\n",
        "\n",
        "        Returns:\n",
        "            ndarray: linear layer result.\n",
        "        '''\n",
        "        self.x = x\n",
        "        return np.matmul(x, self.parameters[0]) + self.parameters[1]\n",
        "\n",
        "    def backward(self, delta):\n",
        "        '''\n",
        "        Args:\n",
        "            delta (ndarray): gradient of L with repect to node's output, dL/dy.\n",
        "\n",
        "        Returns:\n",
        "            ndarray: gradient of L with respect to node's input, dL/dx\n",
        "        '''\n",
        "        self.parameters_deltas[0] = self.x.T.dot(delta)\n",
        "        self.parameters_deltas[1] = np.sum(delta, 0)\n",
        "        return delta.dot(self.parameters[0].T)\n",
        "\n",
        "\n",
        "class Sigmoid(Node):\n",
        "    '''\n",
        "    Sigmoid activation function\n",
        "    '''\n",
        "    def __init__(self):\n",
        "        super(Sigmoid, self).__init__('sigmoid', [])\n",
        "\n",
        "    def forward(self, x, *args):\n",
        "        '''\n",
        "        Args:\n",
        "            x (2darray): input array of shape (batch_size, n_variables).\n",
        "\n",
        "        Returns:\n",
        "            ndarray: sigmoid activation result.\n",
        "        '''\n",
        "        self.x = x\n",
        "        self.y = 1.0 / (1.0 + np.exp(-x))\n",
        "        return self.y\n",
        "\n",
        "    def backward(self, delta):\n",
        "        '''\n",
        "        Args:\n",
        "            delta (ndarray): gradient of L with repect to node's output, dL/dy.\n",
        "\n",
        "        Returns:\n",
        "            ndarray: gradient of L with respect to node's input, dL/dx\n",
        "        '''\n",
        "        return delta * ((1 - self.y) * self.y)\n",
        "\n",
        "\n",
        "class Softmax(Node):\n",
        "    '''\n",
        "    Softmax activation function\n",
        "    '''\n",
        "    def __init__(self):\n",
        "        super(Softmax, self).__init__('softmax', [])\n",
        "\n",
        "    def forward(self, x):\n",
        "        '''\n",
        "        Args:\n",
        "            x (2darray): input array of shape (batch_size, n_variables).\n",
        "\n",
        "        Returns:\n",
        "            ndarray: softmax activation result.\n",
        "        '''\n",
        "        exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
        "        self.y = exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
        "        return self.y\n",
        "\n",
        "    def backward(self, delta):\n",
        "        '''\n",
        "        Args:\n",
        "            delta (ndarray): gradient of L with repect to node's output, dL/dy.\n",
        "\n",
        "        Returns:\n",
        "            ndarray: gradient of L with respect to node's input, dL/dx\n",
        "        '''\n",
        "        return self.y * (delta - np.sum(delta * self.y, axis=1, keepdims=True))\n",
        "\n",
        "\n",
        "class CrossEntropy(Node):\n",
        "    '''\n",
        "    CrossEntropy loss function\n",
        "    '''\n",
        "    def __init__(self):\n",
        "        super(CrossEntropy, self).__init__('cross_entropy', [])\n",
        "\n",
        "    def forward(self, x, l):\n",
        "        '''\n",
        "        Args:\n",
        "            x (2darray): input array of shape (batch_size, n_variables), softmax result.\n",
        "            l (2darray): label array of shape (batch_size, n_variables), one-hot encoded.\n",
        "\n",
        "        Returns:\n",
        "            ndarray: cross entropy loss.\n",
        "        '''\n",
        "        self.x = x\n",
        "        self.l = l\n",
        "        self.y = -np.sum(l * np.log(x + 1e-8)) / x.shape[0]\n",
        "        return self.y\n",
        "\n",
        "    def backward(self, delta=1):\n",
        "        '''\n",
        "        Args:\n",
        "            delta (int): defaults to 1 since the output of cross entropy loss is a scalar.\n",
        "\n",
        "        Returns:\n",
        "            ndarray: gradient of L with respect to node's input, dL/dx\n",
        "        '''\n",
        "        return -self.l / (self.x + 1e-8) / self.x.shape[0]\n",
        "\n",
        "\n",
        "class Mean(Node):\n",
        "    '''\n",
        "    Mean function\n",
        "    '''\n",
        "    def __init__(self):\n",
        "        super(Mean, self).__init__('mean', [])\n",
        "\n",
        "    def forward(self, x):\n",
        "        '''\n",
        "        Args:\n",
        "            x (2darray): input array of shape (batch_size, n_variables).\n",
        "\n",
        "        Returns:\n",
        "            ndarray: mean function result.\n",
        "        '''\n",
        "        self.x = x\n",
        "        return x.mean()\n",
        "\n",
        "    def backward(self, delta):\n",
        "        '''\n",
        "        Args:\n",
        "            delta (ndarray): gradient of L with repect to node's output, dL/dy.\n",
        "\n",
        "        Returns:\n",
        "            ndarray: gradient of L with respect to node's input, dL/dx\n",
        "        '''\n",
        "        return delta * np.ones(self.x.shape) / np.prod(self.x.shape)\n",
        "\n",
        "\n",
        "def load_MNIST():\n",
        "    '''\n",
        "    Download and unpack MNIST data.\n",
        "\n",
        "    Returns:\n",
        "        tuple of ndarray: tuple of length 4. They are training set data, training set label,\n",
        "            test set data and test set label.\n",
        "    '''\n",
        "    base = \"http://yann.lecun.com/exdb/mnist/\"\n",
        "    objects = ['t10k-images-idx3-ubyte', 't10k-labels-idx1-ubyte',\n",
        "               'train-images-idx3-ubyte', 'train-labels-idx1-ubyte']\n",
        "    end = \".gz\"\n",
        "    path = \"data/raw/\"\n",
        "    cmd = [\"mkdir\", \"-p\", path]\n",
        "    subprocess.check_call(cmd)\n",
        "    print('Downloading MNIST dataset. Please do not stop the program\\\n",
        "    during the download. If you do, remove `data` folder and try again.')\n",
        "    for obj in objects:\n",
        "        if not os.path.isfile(path + obj):\n",
        "            cmd = [\"wget\", base + obj + end, \"-P\", path]\n",
        "            subprocess.check_call(cmd)\n",
        "            cmd = [\"gzip\", \"-d\", path + obj + end]\n",
        "            subprocess.check_call(cmd)\n",
        "\n",
        "    def unpack(filename):\n",
        "        '''\n",
        "        Unpack file.\n",
        "        '''\n",
        "        with open(filename, 'rb') as f:\n",
        "            _, _, dims = struct.unpack('>HBB', f.read(4))\n",
        "            shape = tuple(struct.unpack('>I', f.read(4))\n",
        "                          [0] for d in range(dims))\n",
        "            data = np.frombuffer(f.read(), dtype=np.uint8).reshape(shape)\n",
        "            return data\n",
        "\n",
        "    # load objects\n",
        "    data = []\n",
        "    for name in objects:\n",
        "        name = path + name\n",
        "        data.append(unpack(name))\n",
        "    labels = np.zeros([data[1].shape[0], 10])\n",
        "    for i, iterm in enumerate(data[1]):\n",
        "        labels[i][iterm] = 1\n",
        "    data[1] = labels\n",
        "    labels = np.zeros([data[3].shape[0], 10])\n",
        "    for i, iterm in enumerate(data[3]):\n",
        "        labels[i][iterm] = 1\n",
        "    data[3] = labels\n",
        "    return data\n",
        "\n",
        "\n",
        "def random_draw(data, label, batch_size):\n",
        "    '''\n",
        "    Randomly draw.\n",
        "\n",
        "    Args:\n",
        "        data (ndarray): dataset of shape (batch_size, n_variables)\n",
        "        label (ndarray): one-hot label for dataset,\n",
        "            for example, 3 is [0, 0, 0, 1, 0, 0, 0, 0, 0, 0]\n",
        "        batch_size (int): size of batch.\n",
        "\n",
        "    Returns:\n",
        "        2darray: image data batch;\n",
        "        2darray: label of images draw.\n",
        "    '''\n",
        "    perm = np.random.permutation(data.shape[0])\n",
        "    data_b = data[perm[:batch_size]]\n",
        "    label_b = label[perm[:batch_size]]\n",
        "    return data_b.reshape([data_b.shape[0], -1]) / 255.0, label_b\n",
        "\n",
        "def match_ratio(result, label):\n",
        "    '''the ratio of result matching target.'''\n",
        "    label_p = np.argmax(result, axis=1)\n",
        "    label_t = np.argmax(label, axis=1)\n",
        "    ratio = np.sum(label_p == label_t) / label_t.shape[0]\n",
        "    return ratio\n",
        "\n",
        "\n",
        "def net_forward(net, x, label):\n",
        "    '''forward function for this sequencial network.'''\n",
        "    for node in net:\n",
        "        if node.name == 'cross_entropy':\n",
        "            result = x\n",
        "            x = node.forward(x, label)\n",
        "        else:\n",
        "            x = node.forward(x)\n",
        "    return result, x\n",
        "\n",
        "\n",
        "def net_backward(net):\n",
        "    '''backward function for this sequencial network.'''\n",
        "    y_delta = 1.0\n",
        "    for node in net[::-1]:\n",
        "        y_delta = node.backward(y_delta)\n",
        "    return y_delta\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "wiKeajsU1-i5"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "    batch_size = 200\n",
        "    learning_rate = 0.3\n",
        "    dim_img = 784\n",
        "    num_digit = 10\n",
        "    num_epoch = 40\n",
        "    test_data, test_label, train_data, train_label = load_MNIST()\n",
        "    num_iteration = len(train_data) // batch_size\n",
        "\n",
        "    net = [Linear(dim_img, 200), Sigmoid(), Linear(200, 100), Sigmoid(), Linear(100, num_digit), Softmax(), CrossEntropy(), Mean()]\n",
        "\n",
        "    nparams = 0\n",
        "    for term in net:\n",
        "        for para in term.parameters:\n",
        "            nparams += np.prod(para.shape)\n",
        "    print('total number of trainable parameters:', nparams)\n",
        "\n",
        "    x, label = random_draw(test_data, test_label, 1000)\n",
        "    result, loss = net_forward(net, x, label)\n",
        "    print('Before Training.\\nTest loss = %.4f, correct rate = %.3f' % (loss, match_ratio(result, label)))\n",
        "\n",
        "    for epoch in range(num_epoch):\n",
        "        for j in range(num_iteration):\n",
        "            x, label = random_draw(train_data, train_label, batch_size)\n",
        "            result, loss = net_forward(net, x, label)\n",
        "\n",
        "            net_backward(net)\n",
        "\n",
        "            for node in net:\n",
        "                if node.name == 'linear':\n",
        "                    for i in range(len(node.parameters)):\n",
        "                        node.parameters[i] -= learning_rate * node.parameters_deltas[i]\n",
        "\n",
        "        result_test, loss_test = net_forward(net, test_data.reshape(test_data.shape[0], -1) / 255.0, test_label)\n",
        "        print(\"epoch = %d/%d, loss = %.4f, corret rate = %.3f, test correct rate = %.3f\" %\n",
        "              (epoch, num_epoch, loss, match_ratio(result, label), match_ratio(result_test, test_label)))\n",
        "\n",
        "    result_test, loss_test = net_forward(net, test_data.reshape(test_data.shape[0], -1) / 255.0, test_label)\n",
        "    print('Test loss = %.4f, correct rate = %.3f' % (loss_test, match_ratio(result_test, test_label)))\n",
        "\n",
        "    layer_string = []\n",
        "    layer_paramters = []\n",
        "    with open('/content/sample_data/npMnistParameters.npy', 'wb') as f:\n",
        "        for term in net:\n",
        "            layer_string.append(term.name)\n",
        "            if term.name == 'linear':\n",
        "                layer_paramters.append((int(term.parameters[0].shape[0]), int(term.parameters[0].shape[1])))\n",
        "                np.save(f, term.parameters[0])\n",
        "                np.save(f, term.parameters[1])\n",
        "            else:\n",
        "                layer_paramters.append(None)\n",
        "\n",
        "    with open('/content/sample_data/npMnistStructure.json', 'w') as f:\n",
        "        config = {'struct': layer_string, 'num_parametes': layer_paramters}\n",
        "        json.dump(config, f)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "88jI-Fdl5RLE",
        "outputId": "6704771a-f1cd-4da2-cf56-cb949d1301bd"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading MNIST dataset. Please do not stop the program    during the download. If you do, remove `data` folder and try again.\n",
            "total number of trainable parameters: 178110\n",
            "Before Training.\n",
            "Test loss = 2.5237, correct rate = 0.042\n",
            "epoch = 0/40, loss = 0.5267, corret rate = 0.875, test correct rate = 0.856\n",
            "epoch = 1/40, loss = 0.3677, corret rate = 0.870, test correct rate = 0.898\n",
            "epoch = 2/40, loss = 0.3192, corret rate = 0.900, test correct rate = 0.909\n",
            "epoch = 3/40, loss = 0.1949, corret rate = 0.965, test correct rate = 0.915\n",
            "epoch = 4/40, loss = 0.1846, corret rate = 0.930, test correct rate = 0.919\n",
            "epoch = 5/40, loss = 0.2291, corret rate = 0.945, test correct rate = 0.928\n",
            "epoch = 6/40, loss = 0.2168, corret rate = 0.935, test correct rate = 0.931\n",
            "epoch = 7/40, loss = 0.2544, corret rate = 0.910, test correct rate = 0.934\n",
            "epoch = 8/40, loss = 0.1421, corret rate = 0.960, test correct rate = 0.937\n",
            "epoch = 9/40, loss = 0.1619, corret rate = 0.940, test correct rate = 0.942\n",
            "epoch = 10/40, loss = 0.2438, corret rate = 0.925, test correct rate = 0.943\n",
            "epoch = 11/40, loss = 0.1689, corret rate = 0.970, test correct rate = 0.944\n",
            "epoch = 12/40, loss = 0.1815, corret rate = 0.955, test correct rate = 0.950\n",
            "epoch = 13/40, loss = 0.1856, corret rate = 0.925, test correct rate = 0.953\n",
            "epoch = 14/40, loss = 0.1174, corret rate = 0.965, test correct rate = 0.955\n",
            "epoch = 15/40, loss = 0.1997, corret rate = 0.940, test correct rate = 0.953\n",
            "epoch = 16/40, loss = 0.1165, corret rate = 0.955, test correct rate = 0.956\n",
            "epoch = 17/40, loss = 0.1503, corret rate = 0.955, test correct rate = 0.958\n",
            "epoch = 18/40, loss = 0.2707, corret rate = 0.930, test correct rate = 0.960\n",
            "epoch = 19/40, loss = 0.1242, corret rate = 0.980, test correct rate = 0.961\n",
            "epoch = 20/40, loss = 0.0997, corret rate = 0.975, test correct rate = 0.963\n",
            "epoch = 21/40, loss = 0.0929, corret rate = 0.975, test correct rate = 0.963\n",
            "epoch = 22/40, loss = 0.1306, corret rate = 0.980, test correct rate = 0.963\n",
            "epoch = 23/40, loss = 0.0566, corret rate = 0.985, test correct rate = 0.965\n",
            "epoch = 24/40, loss = 0.1431, corret rate = 0.960, test correct rate = 0.964\n",
            "epoch = 25/40, loss = 0.0888, corret rate = 0.975, test correct rate = 0.967\n",
            "epoch = 26/40, loss = 0.0638, corret rate = 0.975, test correct rate = 0.967\n",
            "epoch = 27/40, loss = 0.0566, corret rate = 0.980, test correct rate = 0.968\n",
            "epoch = 28/40, loss = 0.0646, corret rate = 0.985, test correct rate = 0.968\n",
            "epoch = 29/40, loss = 0.0462, corret rate = 0.985, test correct rate = 0.968\n",
            "epoch = 30/40, loss = 0.0572, corret rate = 0.990, test correct rate = 0.969\n",
            "epoch = 31/40, loss = 0.1022, corret rate = 0.970, test correct rate = 0.970\n",
            "epoch = 32/40, loss = 0.0634, corret rate = 0.995, test correct rate = 0.971\n",
            "epoch = 33/40, loss = 0.0559, corret rate = 0.980, test correct rate = 0.970\n",
            "epoch = 34/40, loss = 0.0798, corret rate = 0.980, test correct rate = 0.970\n",
            "epoch = 35/40, loss = 0.0569, corret rate = 0.995, test correct rate = 0.971\n",
            "epoch = 36/40, loss = 0.0664, corret rate = 0.985, test correct rate = 0.972\n",
            "epoch = 37/40, loss = 0.0726, corret rate = 0.980, test correct rate = 0.972\n",
            "epoch = 38/40, loss = 0.0569, corret rate = 0.985, test correct rate = 0.973\n",
            "epoch = 39/40, loss = 0.0797, corret rate = 0.975, test correct rate = 0.973\n",
            "Test loss = 0.0868, correct rate = 0.973\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from mnistClassification import Linear, Sigmoid, Softmax, CrossEntropy, Mean, load_MNIST, match_ratio, net_forward\n",
        "import numpy as np\n",
        "import json\n",
        "\n",
        "\n",
        "train_data, train_label, test_data, test_label = load_MNIST()\n",
        "\n",
        "with open('/content/sample_data/npMnistStructure.json', 'r') as f:\n",
        "    config = json.load(f)\n",
        "\n",
        "net = []\n",
        "with open('/content/sample_data/npMnistParameters.npy', 'rb') as f:\n",
        "    for idx, term in enumerate(config['struct']):\n",
        "        if term == 'Linear':\n",
        "            net.append(Linear(config['num_params'][idx][0], config['num_params'][idx][1]))\n",
        "            parameters = [np.load(f), np.load(f)]\n",
        "            net[-1].parameters = parameters\n",
        "        if term == 'linear':\n",
        "            net.append(Linear(config['num_parametes'][idx][0], config['num_parametes'][idx][1]))\n",
        "            parameters = [np.load(f), np.load(f)]\n",
        "            net[-1].parameters = parameters\n",
        "        elif term == 'sigmoid':\n",
        "            net.append(Sigmoid())\n",
        "        elif term == 'softmax':\n",
        "            net.append(Softmax())\n",
        "        elif term == 'cross_entropy':\n",
        "            net.append(CrossEntropy())\n",
        "        elif term == 'mean':\n",
        "            net.append(Mean())\n",
        "        else:\n",
        "            raise Exception(\"The loaded node name not recognized!\")\n",
        "\n",
        "result_test, loss_test = net_forward(net, test_data.reshape(test_data.shape[0], -1), test_label)\n",
        "print('MNIST test correct rate = %.3f' % (match_ratio(result_test, test_label)))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ScrwyjvE51N6",
        "outputId": "368bbe3d-fb1b-4c8a-8d42-659be193994d"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading MNIST dataset. Please do not stop the program    during the download. If you do, remove `data` folder and try again.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-4-d816217fcb57>:81: RuntimeWarning: overflow encountered in exp\n",
            "  self.y = 1.0 / (1.0 + np.exp(-x))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MNIST test correct rate = 0.964\n"
          ]
        }
      ]
    }
  ]
}